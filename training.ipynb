{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\lab-fikom\\miniconda3\\envs\\akmalEnv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lab-fikom\\miniconda3\\envs\\akmalEnv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lab-fikom\\miniconda3\\envs\\akmalEnv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded base model.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lab-fikom\\miniconda3\\envs\\akmalEnv\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Epoch: [0][0/249]\tBatch Time 16.513 (16.513)\tData Time 10.600 (10.600)\tLoss 17.5161 (17.5161)\t\n",
      "Epoch: [0][200/249]\tBatch Time 0.076 (0.158)\tData Time 0.001 (0.053)\tLoss 4.7807 (6.7912)\t\n",
      "Epoch: [1][0/249]\tBatch Time 11.186 (11.186)\tData Time 10.821 (10.821)\tLoss 4.1055 (4.1055)\t\n",
      "Epoch: [1][200/249]\tBatch Time 0.075 (0.132)\tData Time 0.000 (0.054)\tLoss 3.1812 (4.1423)\t\n",
      "Epoch: [2][0/249]\tBatch Time 10.891 (10.891)\tData Time 10.649 (10.649)\tLoss 3.6943 (3.6943)\t\n",
      "Epoch: [2][200/249]\tBatch Time 0.076 (0.131)\tData Time 0.000 (0.053)\tLoss 3.8931 (3.4772)\t\n",
      "Epoch: [3][0/249]\tBatch Time 11.181 (11.181)\tData Time 10.831 (10.831)\tLoss 3.0866 (3.0866)\t\n",
      "Epoch: [3][200/249]\tBatch Time 0.077 (0.132)\tData Time 0.000 (0.054)\tLoss 3.1616 (3.2014)\t\n",
      "Epoch: [4][0/249]\tBatch Time 10.898 (10.898)\tData Time 10.567 (10.567)\tLoss 3.1551 (3.1551)\t\n",
      "Epoch: [4][200/249]\tBatch Time 0.078 (0.130)\tData Time 0.000 (0.053)\tLoss 2.5149 (2.8035)\t\n",
      "Epoch: [5][0/249]\tBatch Time 10.921 (10.921)\tData Time 10.644 (10.644)\tLoss 2.8238 (2.8238)\t\n",
      "Epoch: [5][200/249]\tBatch Time 0.075 (0.130)\tData Time 0.000 (0.053)\tLoss 2.4848 (2.5767)\t\n",
      "Epoch: [6][0/249]\tBatch Time 10.980 (10.980)\tData Time 10.583 (10.583)\tLoss 2.1873 (2.1873)\t\n",
      "Epoch: [6][200/249]\tBatch Time 0.077 (0.131)\tData Time 0.000 (0.053)\tLoss 2.4184 (2.3464)\t\n",
      "Epoch: [7][0/249]\tBatch Time 10.981 (10.981)\tData Time 10.710 (10.710)\tLoss 2.0058 (2.0058)\t\n",
      "Epoch: [7][200/249]\tBatch Time 0.077 (0.131)\tData Time 0.000 (0.054)\tLoss 2.1035 (2.2817)\t\n",
      "Epoch: [8][0/249]\tBatch Time 11.005 (11.005)\tData Time 10.610 (10.610)\tLoss 2.4514 (2.4514)\t\n",
      "Epoch: [8][200/249]\tBatch Time 0.077 (0.131)\tData Time 0.001 (0.053)\tLoss 2.6186 (2.1544)\t\n",
      "Epoch: [9][0/249]\tBatch Time 11.016 (11.016)\tData Time 10.682 (10.682)\tLoss 2.1057 (2.1057)\t\n",
      "Epoch: [9][200/249]\tBatch Time 0.078 (0.131)\tData Time 0.001 (0.054)\tLoss 2.1756 (2.0782)\t\n",
      "Epoch: [10][0/249]\tBatch Time 10.862 (10.862)\tData Time 10.595 (10.595)\tLoss 2.1499 (2.1499)\t\n",
      "Epoch: [10][200/249]\tBatch Time 0.078 (0.131)\tData Time 0.000 (0.053)\tLoss 1.4610 (2.0084)\t\n",
      "Epoch: [11][0/249]\tBatch Time 10.987 (10.987)\tData Time 10.654 (10.654)\tLoss 1.5208 (1.5208)\t\n",
      "Epoch: [11][200/249]\tBatch Time 0.082 (0.131)\tData Time 0.001 (0.053)\tLoss 2.6881 (1.8726)\t\n",
      "Epoch: [12][0/249]\tBatch Time 11.150 (11.150)\tData Time 10.818 (10.818)\tLoss 1.4285 (1.4285)\t\n",
      "Epoch: [12][200/249]\tBatch Time 0.079 (0.132)\tData Time 0.001 (0.054)\tLoss 2.8650 (2.1490)\t\n",
      "Epoch: [13][0/249]\tBatch Time 11.116 (11.116)\tData Time 10.729 (10.729)\tLoss 2.2653 (2.2653)\t\n",
      "Epoch: [13][200/249]\tBatch Time 0.080 (0.132)\tData Time 0.000 (0.054)\tLoss 1.4416 (2.1259)\t\n",
      "Epoch: [14][0/249]\tBatch Time 10.866 (10.866)\tData Time 10.560 (10.560)\tLoss 1.4932 (1.4932)\t\n",
      "Epoch: [14][200/249]\tBatch Time 0.079 (0.131)\tData Time 0.001 (0.053)\tLoss 1.8805 (1.8619)\t\n",
      "Epoch: [15][0/249]\tBatch Time 11.049 (11.049)\tData Time 10.675 (10.675)\tLoss 3.0080 (3.0080)\t\n",
      "Epoch: [15][200/249]\tBatch Time 0.078 (0.132)\tData Time 0.000 (0.053)\tLoss 2.4774 (1.8560)\t\n",
      "Epoch: [16][0/249]\tBatch Time 10.890 (10.890)\tData Time 10.498 (10.498)\tLoss 1.2889 (1.2889)\t\n",
      "Epoch: [16][200/249]\tBatch Time 0.078 (0.132)\tData Time 0.000 (0.053)\tLoss 1.9583 (1.7996)\t\n",
      "Epoch: [17][0/249]\tBatch Time 10.923 (10.923)\tData Time 10.528 (10.528)\tLoss 1.4362 (1.4362)\t\n",
      "Epoch: [17][200/249]\tBatch Time 0.078 (0.132)\tData Time 0.000 (0.053)\tLoss 1.0982 (1.7571)\t\n",
      "Epoch: [18][0/249]\tBatch Time 11.182 (11.182)\tData Time 10.789 (10.789)\tLoss 1.5722 (1.5722)\t\n",
      "Epoch: [18][200/249]\tBatch Time 0.076 (0.133)\tData Time 0.000 (0.054)\tLoss 1.7616 (1.6250)\t\n",
      "Epoch: [19][0/249]\tBatch Time 10.991 (10.991)\tData Time 10.714 (10.714)\tLoss 2.3046 (2.3046)\t\n",
      "Epoch: [19][200/249]\tBatch Time 0.077 (0.132)\tData Time 0.000 (0.054)\tLoss 1.5734 (1.6207)\t\n",
      "Epoch: [20][0/249]\tBatch Time 10.961 (10.961)\tData Time 10.625 (10.625)\tLoss 1.5932 (1.5932)\t\n",
      "Epoch: [20][200/249]\tBatch Time 0.079 (0.132)\tData Time 0.001 (0.053)\tLoss 1.2788 (1.5330)\t\n",
      "Epoch: [21][0/249]\tBatch Time 10.975 (10.975)\tData Time 10.599 (10.599)\tLoss 1.3798 (1.3798)\t\n",
      "Epoch: [21][200/249]\tBatch Time 0.076 (0.132)\tData Time 0.000 (0.053)\tLoss 1.5363 (1.4634)\t\n",
      "Epoch: [22][0/249]\tBatch Time 11.571 (11.571)\tData Time 11.318 (11.318)\tLoss 1.5489 (1.5489)\t\n",
      "Epoch: [22][200/249]\tBatch Time 0.122 (0.135)\tData Time 0.000 (0.057)\tLoss 1.8551 (1.4927)\t\n",
      "Epoch: [23][0/249]\tBatch Time 10.913 (10.913)\tData Time 10.587 (10.587)\tLoss 1.8176 (1.8176)\t\n",
      "Epoch: [23][200/249]\tBatch Time 0.078 (0.132)\tData Time 0.000 (0.053)\tLoss 1.0200 (1.4880)\t\n",
      "Epoch: [24][0/249]\tBatch Time 10.990 (10.990)\tData Time 10.726 (10.726)\tLoss 1.3493 (1.3493)\t\n",
      "Epoch: [24][200/249]\tBatch Time 0.077 (0.132)\tData Time 0.000 (0.054)\tLoss 1.4056 (1.4857)\t\n",
      "Epoch: [25][0/249]\tBatch Time 10.898 (10.898)\tData Time 10.584 (10.584)\tLoss 1.5059 (1.5059)\t\n",
      "Epoch: [25][200/249]\tBatch Time 0.078 (0.132)\tData Time 0.000 (0.053)\tLoss 1.0985 (1.3529)\t\n",
      "Epoch: [26][0/249]\tBatch Time 10.972 (10.972)\tData Time 10.578 (10.578)\tLoss 1.5462 (1.5462)\t\n",
      "Epoch: [26][200/249]\tBatch Time 0.078 (0.133)\tData Time 0.001 (0.053)\tLoss 1.2634 (1.3384)\t\n",
      "Epoch: [27][0/249]\tBatch Time 10.899 (10.899)\tData Time 10.554 (10.554)\tLoss 1.3141 (1.3141)\t\n",
      "Epoch: [27][200/249]\tBatch Time 0.079 (0.132)\tData Time 0.001 (0.053)\tLoss 1.4423 (1.3419)\t\n",
      "Epoch: [28][0/249]\tBatch Time 10.984 (10.984)\tData Time 10.718 (10.718)\tLoss 2.0735 (2.0735)\t\n",
      "Epoch: [28][200/249]\tBatch Time 0.078 (0.133)\tData Time 0.001 (0.054)\tLoss 1.0425 (1.2976)\t\n",
      "Epoch: [29][0/249]\tBatch Time 10.902 (10.902)\tData Time 10.612 (10.612)\tLoss 0.9949 (0.9949)\t\n",
      "Epoch: [29][200/249]\tBatch Time 0.076 (0.131)\tData Time 0.000 (0.053)\tLoss 1.5687 (1.2654)\t\n",
      "Epoch: [30][0/249]\tBatch Time 11.065 (11.065)\tData Time 10.740 (10.740)\tLoss 1.9830 (1.9830)\t\n",
      "Epoch: [30][200/249]\tBatch Time 0.078 (0.133)\tData Time 0.001 (0.054)\tLoss 1.3977 (1.3215)\t\n",
      "Epoch: [31][0/249]\tBatch Time 10.888 (10.888)\tData Time 10.598 (10.598)\tLoss 0.7004 (0.7004)\t\n",
      "Epoch: [31][200/249]\tBatch Time 0.078 (0.132)\tData Time 0.001 (0.053)\tLoss 1.1071 (1.3925)\t\n",
      "Epoch: [32][0/249]\tBatch Time 11.027 (11.027)\tData Time 10.621 (10.621)\tLoss 1.1415 (1.1415)\t\n",
      "Epoch: [32][200/249]\tBatch Time 0.079 (0.133)\tData Time 0.000 (0.053)\tLoss 2.0150 (1.2516)\t\n",
      "Epoch: [33][0/249]\tBatch Time 11.071 (11.071)\tData Time 10.684 (10.684)\tLoss 2.0696 (2.0696)\t\n",
      "Epoch: [33][200/249]\tBatch Time 0.077 (0.135)\tData Time 0.001 (0.054)\tLoss 1.8777 (1.2718)\t\n",
      "Epoch: [34][0/249]\tBatch Time 10.945 (10.945)\tData Time 10.598 (10.598)\tLoss 1.2973 (1.2973)\t\n",
      "Epoch: [34][200/249]\tBatch Time 0.078 (0.132)\tData Time 0.000 (0.053)\tLoss 1.3618 (1.2330)\t\n",
      "Epoch: [35][0/249]\tBatch Time 10.912 (10.912)\tData Time 10.651 (10.651)\tLoss 1.3507 (1.3507)\t\n",
      "Epoch: [35][200/249]\tBatch Time 0.078 (0.132)\tData Time 0.000 (0.053)\tLoss 2.0531 (1.2712)\t\n",
      "Epoch: [36][0/249]\tBatch Time 11.017 (11.017)\tData Time 10.623 (10.623)\tLoss 1.0890 (1.0890)\t\n",
      "Epoch: [36][200/249]\tBatch Time 0.080 (0.133)\tData Time 0.000 (0.053)\tLoss 0.8700 (1.1919)\t\n",
      "Epoch: [37][0/249]\tBatch Time 10.960 (10.960)\tData Time 10.689 (10.689)\tLoss 1.1776 (1.1776)\t\n",
      "Epoch: [37][200/249]\tBatch Time 0.077 (0.133)\tData Time 0.000 (0.054)\tLoss 1.1345 (1.1353)\t\n",
      "Epoch: [38][0/249]\tBatch Time 11.044 (11.044)\tData Time 10.667 (10.667)\tLoss 1.1971 (1.1971)\t\n",
      "Epoch: [38][200/249]\tBatch Time 0.079 (0.133)\tData Time 0.001 (0.054)\tLoss 0.9765 (1.1557)\t\n",
      "Epoch: [39][0/249]\tBatch Time 10.920 (10.920)\tData Time 10.658 (10.658)\tLoss 0.9119 (0.9119)\t\n",
      "Epoch: [39][200/249]\tBatch Time 0.079 (0.132)\tData Time 0.000 (0.053)\tLoss 0.7645 (1.1536)\t\n",
      "Epoch: [40][0/249]\tBatch Time 11.087 (11.087)\tData Time 10.749 (10.749)\tLoss 0.9440 (0.9440)\t\n",
      "Epoch: [40][200/249]\tBatch Time 0.076 (0.133)\tData Time 0.000 (0.054)\tLoss 1.2937 (1.1065)\t\n",
      "Epoch: [41][0/249]\tBatch Time 11.081 (11.081)\tData Time 10.683 (10.683)\tLoss 1.3491 (1.3491)\t\n",
      "Epoch: [41][200/249]\tBatch Time 0.079 (0.133)\tData Time 0.000 (0.054)\tLoss 0.8076 (1.1756)\t\n",
      "Epoch: [42][0/249]\tBatch Time 10.867 (10.867)\tData Time 10.544 (10.544)\tLoss 1.0356 (1.0356)\t\n",
      "Epoch: [42][200/249]\tBatch Time 0.088 (0.132)\tData Time 0.000 (0.053)\tLoss 0.9125 (1.1725)\t\n",
      "Epoch: [43][0/249]\tBatch Time 10.795 (10.795)\tData Time 10.500 (10.500)\tLoss 0.7122 (0.7122)\t\n",
      "Epoch: [43][200/249]\tBatch Time 0.080 (0.131)\tData Time 0.000 (0.053)\tLoss 0.7173 (1.0361)\t\n",
      "Epoch: [44][0/249]\tBatch Time 10.797 (10.797)\tData Time 10.406 (10.406)\tLoss 0.8874 (0.8874)\t\n",
      "Epoch: [44][200/249]\tBatch Time 0.072 (0.131)\tData Time 0.000 (0.053)\tLoss 0.8995 (1.0780)\t\n",
      "Epoch: [45][0/249]\tBatch Time 10.862 (10.862)\tData Time 10.547 (10.547)\tLoss 1.0973 (1.0973)\t\n",
      "Epoch: [45][200/249]\tBatch Time 0.088 (0.132)\tData Time 0.000 (0.053)\tLoss 0.9433 (1.1193)\t\n",
      "Epoch: [46][0/249]\tBatch Time 10.766 (10.766)\tData Time 10.500 (10.500)\tLoss 1.0446 (1.0446)\t\n",
      "Epoch: [46][200/249]\tBatch Time 0.078 (0.131)\tData Time 0.000 (0.053)\tLoss 1.0232 (1.1276)\t\n",
      "Epoch: [47][0/249]\tBatch Time 10.822 (10.822)\tData Time 10.416 (10.416)\tLoss 1.7507 (1.7507)\t\n",
      "Epoch: [47][200/249]\tBatch Time 0.080 (0.132)\tData Time 0.000 (0.052)\tLoss 0.7420 (1.0321)\t\n",
      "Epoch: [48][0/249]\tBatch Time 10.594 (10.594)\tData Time 10.313 (10.313)\tLoss 1.1201 (1.1201)\t\n",
      "Epoch: [48][200/249]\tBatch Time 0.078 (0.131)\tData Time 0.000 (0.052)\tLoss 0.7265 (1.0956)\t\n",
      "Epoch: [49][0/249]\tBatch Time 10.882 (10.882)\tData Time 10.642 (10.642)\tLoss 0.9627 (0.9627)\t\n",
      "Epoch: [49][200/249]\tBatch Time 0.077 (0.132)\tData Time 0.000 (0.053)\tLoss 1.0860 (1.0404)\t\n",
      "Epoch: [50][0/249]\tBatch Time 10.905 (10.905)\tData Time 10.641 (10.641)\tLoss 0.8816 (0.8816)\t\n",
      "Epoch: [50][200/249]\tBatch Time 0.079 (0.132)\tData Time 0.001 (0.053)\tLoss 0.9309 (1.0295)\t\n",
      "Epoch: [51][0/249]\tBatch Time 10.844 (10.844)\tData Time 10.472 (10.472)\tLoss 0.9395 (0.9395)\t\n",
      "Epoch: [51][200/249]\tBatch Time 0.082 (0.131)\tData Time 0.001 (0.052)\tLoss 0.7637 (1.0116)\t\n",
      "Epoch: [52][0/249]\tBatch Time 10.799 (10.799)\tData Time 10.535 (10.535)\tLoss 1.3021 (1.3021)\t\n",
      "Epoch: [52][200/249]\tBatch Time 0.080 (0.131)\tData Time 0.001 (0.053)\tLoss 0.8143 (1.0435)\t\n",
      "Epoch: [53][0/249]\tBatch Time 10.924 (10.924)\tData Time 10.623 (10.623)\tLoss 0.9670 (0.9670)\t\n",
      "Epoch: [53][200/249]\tBatch Time 0.079 (0.132)\tData Time 0.000 (0.053)\tLoss 0.7175 (1.0111)\t\n",
      "Epoch: [54][0/249]\tBatch Time 11.088 (11.088)\tData Time 10.751 (10.751)\tLoss 0.9293 (0.9293)\t\n",
      "Epoch: [54][200/249]\tBatch Time 0.077 (0.133)\tData Time 0.000 (0.054)\tLoss 1.0066 (1.0292)\t\n",
      "Epoch: [55][0/249]\tBatch Time 10.888 (10.888)\tData Time 10.558 (10.558)\tLoss 0.8628 (0.8628)\t\n",
      "Epoch: [55][200/249]\tBatch Time 0.079 (0.132)\tData Time 0.001 (0.053)\tLoss 2.2899 (0.9928)\t\n",
      "Epoch: [56][0/249]\tBatch Time 10.874 (10.874)\tData Time 10.591 (10.591)\tLoss 1.1827 (1.1827)\t\n",
      "Epoch: [56][200/249]\tBatch Time 0.077 (0.132)\tData Time 0.000 (0.053)\tLoss 0.7903 (0.9986)\t\n",
      "Epoch: [57][0/249]\tBatch Time 11.065 (11.065)\tData Time 10.737 (10.737)\tLoss 1.3286 (1.3286)\t\n",
      "Epoch: [57][200/249]\tBatch Time 0.085 (0.134)\tData Time 0.000 (0.054)\tLoss 0.8682 (1.0143)\t\n",
      "Epoch: [58][0/249]\tBatch Time 10.935 (10.935)\tData Time 10.541 (10.541)\tLoss 1.1373 (1.1373)\t\n",
      "Epoch: [58][200/249]\tBatch Time 0.078 (0.132)\tData Time 0.001 (0.053)\tLoss 1.3900 (0.9832)\t\n",
      "Epoch: [59][0/249]\tBatch Time 10.946 (10.946)\tData Time 10.553 (10.553)\tLoss 1.0227 (1.0227)\t\n",
      "Epoch: [59][200/249]\tBatch Time 0.077 (0.132)\tData Time 0.000 (0.053)\tLoss 0.8259 (1.0192)\t\n",
      "Epoch: [60][0/249]\tBatch Time 10.754 (10.754)\tData Time 10.436 (10.436)\tLoss 0.8775 (0.8775)\t\n",
      "Epoch: [60][200/249]\tBatch Time 0.079 (0.132)\tData Time 0.000 (0.052)\tLoss 0.9492 (0.9658)\t\n",
      "Epoch: [61][0/249]\tBatch Time 10.866 (10.866)\tData Time 10.583 (10.583)\tLoss 1.6136 (1.6136)\t\n",
      "Epoch: [61][200/249]\tBatch Time 0.079 (0.132)\tData Time 0.000 (0.053)\tLoss 0.7478 (0.9538)\t\n",
      "Epoch: [62][0/249]\tBatch Time 10.920 (10.920)\tData Time 10.530 (10.530)\tLoss 1.0680 (1.0680)\t\n",
      "Epoch: [62][200/249]\tBatch Time 0.080 (0.132)\tData Time 0.000 (0.053)\tLoss 0.9816 (0.9721)\t\n",
      "Epoch: [63][0/249]\tBatch Time 10.930 (10.930)\tData Time 10.635 (10.635)\tLoss 0.8473 (0.8473)\t\n",
      "Epoch: [63][200/249]\tBatch Time 0.080 (0.132)\tData Time 0.001 (0.053)\tLoss 0.8801 (0.9341)\t\n",
      "Epoch: [64][0/249]\tBatch Time 10.886 (10.886)\tData Time 10.492 (10.492)\tLoss 1.0288 (1.0288)\t\n",
      "Epoch: [64][200/249]\tBatch Time 0.079 (0.132)\tData Time 0.000 (0.053)\tLoss 0.7462 (0.9222)\t\n",
      "Epoch: [65][0/249]\tBatch Time 10.940 (10.940)\tData Time 10.599 (10.599)\tLoss 0.7520 (0.7520)\t\n",
      "Epoch: [65][200/249]\tBatch Time 0.078 (0.132)\tData Time 0.000 (0.053)\tLoss 0.7270 (0.9132)\t\n",
      "Epoch: [66][0/249]\tBatch Time 10.757 (10.757)\tData Time 10.445 (10.445)\tLoss 0.9959 (0.9959)\t\n",
      "Epoch: [66][200/249]\tBatch Time 0.078 (0.131)\tData Time 0.000 (0.052)\tLoss 0.9958 (0.9528)\t\n",
      "Epoch: [67][0/249]\tBatch Time 10.755 (10.755)\tData Time 10.487 (10.487)\tLoss 0.8000 (0.8000)\t\n",
      "Epoch: [67][200/249]\tBatch Time 0.082 (0.131)\tData Time 0.000 (0.053)\tLoss 0.7624 (0.8852)\t\n",
      "Epoch: [68][0/249]\tBatch Time 10.856 (10.856)\tData Time 10.455 (10.455)\tLoss 0.8826 (0.8826)\t\n",
      "Epoch: [68][200/249]\tBatch Time 0.077 (0.132)\tData Time 0.000 (0.052)\tLoss 0.7467 (0.9041)\t\n",
      "Epoch: [69][0/249]\tBatch Time 11.013 (11.013)\tData Time 10.621 (10.621)\tLoss 0.5703 (0.5703)\t\n",
      "Epoch: [69][200/249]\tBatch Time 0.078 (0.133)\tData Time 0.000 (0.053)\tLoss 1.0650 (0.8714)\t\n",
      "Epoch: [70][0/249]\tBatch Time 11.116 (11.116)\tData Time 10.828 (10.828)\tLoss 0.8035 (0.8035)\t\n",
      "Epoch: [70][200/249]\tBatch Time 0.080 (0.133)\tData Time 0.001 (0.054)\tLoss 1.9960 (0.9609)\t\n",
      "Epoch: [71][0/249]\tBatch Time 10.998 (10.998)\tData Time 10.757 (10.757)\tLoss 0.5852 (0.5852)\t\n",
      "Epoch: [71][200/249]\tBatch Time 0.077 (0.133)\tData Time 0.000 (0.054)\tLoss 0.6747 (0.9608)\t\n",
      "Epoch: [72][0/249]\tBatch Time 10.888 (10.888)\tData Time 10.552 (10.552)\tLoss 0.7140 (0.7140)\t\n",
      "Epoch: [72][200/249]\tBatch Time 0.078 (0.132)\tData Time 0.000 (0.053)\tLoss 1.0470 (0.9405)\t\n",
      "Epoch: [73][0/249]\tBatch Time 10.891 (10.891)\tData Time 10.583 (10.583)\tLoss 0.7489 (0.7489)\t\n",
      "Epoch: [73][200/249]\tBatch Time 0.078 (0.132)\tData Time 0.001 (0.053)\tLoss 1.1884 (0.9326)\t\n",
      "Epoch: [74][0/249]\tBatch Time 10.865 (10.865)\tData Time 10.569 (10.569)\tLoss 0.8071 (0.8071)\t\n",
      "Epoch: [74][200/249]\tBatch Time 0.078 (0.132)\tData Time 0.001 (0.053)\tLoss 1.0752 (0.9047)\t\n",
      "Epoch: [75][0/249]\tBatch Time 10.951 (10.951)\tData Time 10.621 (10.621)\tLoss 0.7115 (0.7115)\t\n",
      "Epoch: [75][200/249]\tBatch Time 0.081 (0.132)\tData Time 0.000 (0.053)\tLoss 0.6346 (0.9411)\t\n",
      "Epoch: [76][0/249]\tBatch Time 10.945 (10.945)\tData Time 10.612 (10.612)\tLoss 1.1177 (1.1177)\t\n",
      "Epoch: [76][200/249]\tBatch Time 0.079 (0.132)\tData Time 0.000 (0.053)\tLoss 0.6571 (0.9141)\t\n",
      "Epoch: [77][0/249]\tBatch Time 10.913 (10.913)\tData Time 10.540 (10.540)\tLoss 0.5022 (0.5022)\t\n",
      "Epoch: [77][200/249]\tBatch Time 0.077 (0.132)\tData Time 0.000 (0.053)\tLoss 0.9213 (0.8817)\t\n",
      "Epoch: [78][0/249]\tBatch Time 11.228 (11.228)\tData Time 10.837 (10.837)\tLoss 0.7121 (0.7121)\t\n",
      "Epoch: [78][200/249]\tBatch Time 0.077 (0.134)\tData Time 0.000 (0.054)\tLoss 0.6108 (0.8645)\t\n",
      "Epoch: [79][0/249]\tBatch Time 11.231 (11.231)\tData Time 10.834 (10.834)\tLoss 0.8989 (0.8989)\t\n",
      "Epoch: [79][200/249]\tBatch Time 0.078 (0.134)\tData Time 0.000 (0.054)\tLoss 0.7433 (0.9009)\t\n",
      "Epoch: [80][0/249]\tBatch Time 11.127 (11.127)\tData Time 10.862 (10.862)\tLoss 1.1174 (1.1174)\t\n",
      "Epoch: [80][200/249]\tBatch Time 0.078 (0.133)\tData Time 0.000 (0.054)\tLoss 0.9948 (0.8660)\t\n",
      "Epoch: [81][0/249]\tBatch Time 11.087 (11.087)\tData Time 10.810 (10.810)\tLoss 0.6419 (0.6419)\t\n",
      "Epoch: [81][200/249]\tBatch Time 0.077 (0.133)\tData Time 0.001 (0.054)\tLoss 0.6078 (0.8859)\t\n",
      "Epoch: [82][0/249]\tBatch Time 11.139 (11.139)\tData Time 10.800 (10.800)\tLoss 0.8041 (0.8041)\t\n",
      "Epoch: [82][200/249]\tBatch Time 0.078 (0.133)\tData Time 0.000 (0.054)\tLoss 0.5119 (0.8499)\t\n",
      "Epoch: [83][0/249]\tBatch Time 10.926 (10.926)\tData Time 10.660 (10.660)\tLoss 0.4895 (0.4895)\t\n",
      "Epoch: [83][200/249]\tBatch Time 0.079 (0.132)\tData Time 0.001 (0.053)\tLoss 1.1152 (0.8748)\t\n",
      "Epoch: [84][0/249]\tBatch Time 11.126 (11.126)\tData Time 10.798 (10.798)\tLoss 0.7439 (0.7439)\t\n",
      "Epoch: [84][200/249]\tBatch Time 0.077 (0.133)\tData Time 0.001 (0.054)\tLoss 0.7627 (0.8836)\t\n",
      "Epoch: [85][0/249]\tBatch Time 11.474 (11.474)\tData Time 11.100 (11.100)\tLoss 0.7878 (0.7878)\t\n",
      "Epoch: [85][200/249]\tBatch Time 0.080 (0.135)\tData Time 0.000 (0.056)\tLoss 1.8325 (0.8938)\t\n",
      "Epoch: [86][0/249]\tBatch Time 10.841 (10.841)\tData Time 10.536 (10.536)\tLoss 0.7514 (0.7514)\t\n",
      "Epoch: [86][200/249]\tBatch Time 0.080 (0.132)\tData Time 0.001 (0.053)\tLoss 0.8351 (0.8609)\t\n",
      "Epoch: [87][0/249]\tBatch Time 10.879 (10.879)\tData Time 10.565 (10.565)\tLoss 0.5136 (0.5136)\t\n",
      "Epoch: [87][200/249]\tBatch Time 0.079 (0.132)\tData Time 0.001 (0.053)\tLoss 0.7644 (0.8789)\t\n",
      "Epoch: [88][0/249]\tBatch Time 10.921 (10.921)\tData Time 10.528 (10.528)\tLoss 0.8753 (0.8753)\t\n",
      "Epoch: [88][200/249]\tBatch Time 0.077 (0.132)\tData Time 0.000 (0.053)\tLoss 0.6194 (0.8600)\t\n",
      "Epoch: [89][0/249]\tBatch Time 10.943 (10.943)\tData Time 10.565 (10.565)\tLoss 1.1464 (1.1464)\t\n",
      "Epoch: [89][200/249]\tBatch Time 0.080 (0.132)\tData Time 0.000 (0.053)\tLoss 1.7950 (0.9210)\t\n",
      "Epoch: [90][0/249]\tBatch Time 10.889 (10.889)\tData Time 10.580 (10.580)\tLoss 0.6331 (0.6331)\t\n",
      "Epoch: [90][200/249]\tBatch Time 0.079 (0.132)\tData Time 0.000 (0.053)\tLoss 0.9463 (0.8271)\t\n",
      "Epoch: [91][0/249]\tBatch Time 10.915 (10.915)\tData Time 10.519 (10.519)\tLoss 0.6490 (0.6490)\t\n",
      "Epoch: [91][200/249]\tBatch Time 0.079 (0.132)\tData Time 0.001 (0.053)\tLoss 1.5901 (0.8123)\t\n",
      "Epoch: [92][0/249]\tBatch Time 10.936 (10.936)\tData Time 10.545 (10.545)\tLoss 0.8573 (0.8573)\t\n",
      "Epoch: [92][200/249]\tBatch Time 0.078 (0.132)\tData Time 0.000 (0.053)\tLoss 0.7784 (0.8898)\t\n",
      "Epoch: [93][0/249]\tBatch Time 10.910 (10.910)\tData Time 10.629 (10.629)\tLoss 0.6116 (0.6116)\t\n",
      "Epoch: [93][200/249]\tBatch Time 0.078 (0.132)\tData Time 0.000 (0.053)\tLoss 0.5423 (0.8803)\t\n",
      "Epoch: [94][0/249]\tBatch Time 10.937 (10.937)\tData Time 10.564 (10.564)\tLoss 0.5739 (0.5739)\t\n",
      "Epoch: [94][200/249]\tBatch Time 0.078 (0.132)\tData Time 0.000 (0.053)\tLoss 0.8869 (0.8505)\t\n",
      "Epoch: [95][0/249]\tBatch Time 10.950 (10.950)\tData Time 10.611 (10.611)\tLoss 1.3372 (1.3372)\t\n",
      "Epoch: [95][200/249]\tBatch Time 0.078 (0.132)\tData Time 0.001 (0.053)\tLoss 0.7383 (0.8276)\t\n",
      "Epoch: [96][0/249]\tBatch Time 11.072 (11.072)\tData Time 10.681 (10.681)\tLoss 0.5595 (0.5595)\t\n",
      "Epoch: [96][200/249]\tBatch Time 0.077 (0.133)\tData Time 0.000 (0.054)\tLoss 0.8003 (0.8460)\t\n",
      "Epoch: [97][0/249]\tBatch Time 10.911 (10.911)\tData Time 10.558 (10.558)\tLoss 0.7779 (0.7779)\t\n",
      "Epoch: [97][200/249]\tBatch Time 0.077 (0.132)\tData Time 0.000 (0.053)\tLoss 0.8744 (0.7987)\t\n",
      "Epoch: [98][0/249]\tBatch Time 10.881 (10.881)\tData Time 10.490 (10.490)\tLoss 0.5963 (0.5963)\t\n",
      "Epoch: [98][200/249]\tBatch Time 0.077 (0.132)\tData Time 0.000 (0.053)\tLoss 0.8348 (0.8017)\t\n",
      "Epoch: [99][0/249]\tBatch Time 10.897 (10.897)\tData Time 10.527 (10.527)\tLoss 0.6736 (0.6736)\t\n",
      "Epoch: [99][200/249]\tBatch Time 0.079 (0.132)\tData Time 0.000 (0.053)\tLoss 0.6805 (0.7676)\t\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "from model import SSD300, MultiBoxLoss\n",
    "from datasets import Dataset\n",
    "from utils import *\n",
    "import torch.utils.tensorboard as tensorboard\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Data parameters\n",
    "data_folder = './'  # folder with data files\n",
    "keep_difficult = True  # use objects considered difficult to detect?\n",
    "\n",
    "# Model parameters\n",
    "# Not too many here since the SSD300 has a very specific structure\n",
    "n_classes = len(label_map)  # number of different types of objects\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Learning parameters\n",
    "checkpoint = None  # path to model checkpoint, None if none\n",
    "batch_size = 8  # batch size\n",
    "iterations = 100  # number of iterations to train\n",
    "workers = 4  # number of workers for loading data in the DataLoader\n",
    "print_freq = 200  # print training status every __ batches\n",
    "lr = 1e-3  # learning rate\n",
    "decay_lr_at = [80000, 100000]  # decay learning rate after these many iterations\n",
    "decay_lr_to = 0.1  # decay learning rate to this fraction of the existing learning rate\n",
    "momentum = 0.9  # momentum\n",
    "weight_decay = 5e-4  # weight decay\n",
    "grad_clip = None  # clip if gradients are exploding, which may happen at larger batch sizes (sometimes at 32) - you will recognize it by a sorting error in the MuliBox loss calculation\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Training.\n",
    "    \"\"\"\n",
    "    global start_epoch, label_map, epoch, checkpoint, decay_lr_at\n",
    "\n",
    "    # Initialize model or load checkpoint\n",
    "    if checkpoint is None:\n",
    "        start_epoch = 0\n",
    "        model = SSD300(n_classes=n_classes)\n",
    "        # Initialize the optimizer, with twice the default learning rate for biases, as in the original Caffe repo\n",
    "        biases = list()\n",
    "        not_biases = list()\n",
    "        for param_name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if param_name.endswith('.bias'):\n",
    "                    biases.append(param)\n",
    "                else:\n",
    "                    not_biases.append(param)\n",
    "        optimizer = torch.optim.SGD(params=[{'params': biases, 'lr': 2 * lr}, {'params': not_biases}],\n",
    "                                    lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print('\\nLoaded checkpoint from epoch %d.\\n' % start_epoch)\n",
    "        model = checkpoint['model']\n",
    "        optimizer = checkpoint['optimizer']\n",
    "\n",
    "    # Move to default device\n",
    "    model = model.to(device)\n",
    "    criterion = MultiBoxLoss(priors_cxcy=model.priors_cxcy).to(device)\n",
    "\n",
    "    # Custom dataloaders\n",
    "    train_dataset = Dataset(data_folder,\n",
    "                                     split='train',\n",
    "                                     keep_difficult=keep_difficult)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                               collate_fn=train_dataset.collate_fn, num_workers=workers,\n",
    "                                               pin_memory=True)  # note that we're passing the collate function here\n",
    "\n",
    "    # Calculate total number of epochs to train and the epochs to decay learning rate at (i.e. convert iterations to epochs)\n",
    "    # To convert iterations to epochs, divide iterations by the number of iterations per epoch\n",
    "    # The paper trains for 120,000 iterations with a batch size of 32, decays after 80,000 and 100,000 iterations\n",
    "    epochs = iterations  # // (len(train_dataset) // 32)\n",
    "    decay_lr_at = [it // (len(train_dataset) // 32) for it in decay_lr_at]\n",
    "\n",
    "    # Initialize TensorBoard\n",
    "    writer = tensorboard.SummaryWriter()\n",
    "    # Epochs\n",
    "    print(epochs)\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "\n",
    "        # Decay learning rate at particular epochs\n",
    "        if epoch in decay_lr_at:\n",
    "            adjust_learning_rate(optimizer, decay_lr_to)\n",
    "\n",
    "        # One epoch's training\n",
    "        train(train_loader=train_loader,\n",
    "              model=model,\n",
    "              criterion=criterion,\n",
    "              optimizer=optimizer,\n",
    "              epoch=epoch)\n",
    "\n",
    "        # Write metrics to TensorBoard\n",
    "        # writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "        # writer.add_scalar('Accuracy/train', train_accuracy, epoch)\n",
    "        # writer.add_scalar('Precision/train', train_precision, epoch)\n",
    "        # writer.add_scalar('Recall/train', train_recall, epoch)\n",
    "        # writer.add_scalar('F1/train', train_f1, epoch)\n",
    "        # Save checkpoint\n",
    "        save_checkpoint(epoch, model, optimizer)\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    One epoch's training.\n",
    "\n",
    "    :param train_loader: DataLoader for training data\n",
    "    :param model: model\n",
    "    :param criterion: MultiBox loss\n",
    "    :param optimizer: optimizer\n",
    "    :param epoch: epoch number\n",
    "    \"\"\"\n",
    "    model.train()  # training mode enables dropout\n",
    "\n",
    "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
    "    data_time = AverageMeter()  # data loading time\n",
    "    losses = AverageMeter()  # loss\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Batches\n",
    "    for i, (images, boxes, labels, _) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - start)\n",
    "\n",
    "        # Move to default device\n",
    "        images = images.to(device)  # (batch_size (N), 3, 300, 300)\n",
    "        boxes = [b.to(device) for b in boxes]\n",
    "        labels = [l.to(device) for l in labels]\n",
    "\n",
    "        # Forward prop.\n",
    "        predicted_locs, predicted_scores = model(images)  # (N, 8732, 4), (N, 8732, n_classes)\n",
    "\n",
    "        # Loss\n",
    "        loss = criterion(predicted_locs, predicted_scores, boxes, labels)  # scalar\n",
    "\n",
    "        # Backward prop.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients, if necessary\n",
    "        if grad_clip is not None:\n",
    "            clip_gradient(optimizer, grad_clip)\n",
    "\n",
    "        # Update model\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        batch_time.update(time.time() - start)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # Print status\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(epoch, i, len(train_loader),\n",
    "                                                                  batch_time=batch_time,\n",
    "                                                                  data_time=data_time, loss=losses))\n",
    "    del predicted_locs, predicted_scores, images, boxes, labels  # free some memory since their histories may be stored\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "akmalEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
